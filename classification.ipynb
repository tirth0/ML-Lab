{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import tree\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import learning_curve\n",
    "import time\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "raw_data = load_wine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "          alcohol  malic_acid         ash  alcalinity_of_ash   magnesium  \\\n",
       "count  178.000000  178.000000  178.000000         178.000000  178.000000   \n",
       "mean    13.000618    2.336348    2.366517          19.494944   99.741573   \n",
       "std      0.811827    1.117146    0.274344           3.339564   14.282484   \n",
       "min     11.030000    0.740000    1.360000          10.600000   70.000000   \n",
       "25%     12.362500    1.602500    2.210000          17.200000   88.000000   \n",
       "50%     13.050000    1.865000    2.360000          19.500000   98.000000   \n",
       "75%     13.677500    3.082500    2.557500          21.500000  107.000000   \n",
       "max     14.830000    5.800000    3.230000          30.000000  162.000000   \n",
       "\n",
       "       total_phenols  flavanoids  nonflavanoid_phenols  proanthocyanins  \\\n",
       "count     178.000000  178.000000            178.000000       178.000000   \n",
       "mean        2.295112    2.029270              0.361854         1.590899   \n",
       "std         0.625851    0.998859              0.124453         0.572359   \n",
       "min         0.980000    0.340000              0.130000         0.410000   \n",
       "25%         1.742500    1.205000              0.270000         1.250000   \n",
       "50%         2.355000    2.135000              0.340000         1.555000   \n",
       "75%         2.800000    2.875000              0.437500         1.950000   \n",
       "max         3.880000    5.080000              0.660000         3.580000   \n",
       "\n",
       "       color_intensity         hue  od280/od315_of_diluted_wines      proline  \\\n",
       "count       178.000000  178.000000                    178.000000   178.000000   \n",
       "mean          5.058090    0.957449                      2.611685   746.893258   \n",
       "std           2.318286    0.228572                      0.709990   314.907474   \n",
       "min           1.280000    0.480000                      1.270000   278.000000   \n",
       "25%           3.220000    0.782500                      1.937500   500.500000   \n",
       "50%           4.690000    0.965000                      2.780000   673.500000   \n",
       "75%           6.200000    1.120000                      3.170000   985.000000   \n",
       "max          13.000000    1.710000                      4.000000  1680.000000   \n",
       "\n",
       "           target  \n",
       "count  178.000000  \n",
       "mean     0.938202  \n",
       "std      0.775035  \n",
       "min      0.000000  \n",
       "25%      0.000000  \n",
       "50%      1.000000  \n",
       "75%      2.000000  \n",
       "max      2.000000  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>alcohol</th>\n      <th>malic_acid</th>\n      <th>ash</th>\n      <th>alcalinity_of_ash</th>\n      <th>magnesium</th>\n      <th>total_phenols</th>\n      <th>flavanoids</th>\n      <th>nonflavanoid_phenols</th>\n      <th>proanthocyanins</th>\n      <th>color_intensity</th>\n      <th>hue</th>\n      <th>od280/od315_of_diluted_wines</th>\n      <th>proline</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>178.000000</td>\n      <td>178.000000</td>\n      <td>178.000000</td>\n      <td>178.000000</td>\n      <td>178.000000</td>\n      <td>178.000000</td>\n      <td>178.000000</td>\n      <td>178.000000</td>\n      <td>178.000000</td>\n      <td>178.000000</td>\n      <td>178.000000</td>\n      <td>178.000000</td>\n      <td>178.000000</td>\n      <td>178.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>13.000618</td>\n      <td>2.336348</td>\n      <td>2.366517</td>\n      <td>19.494944</td>\n      <td>99.741573</td>\n      <td>2.295112</td>\n      <td>2.029270</td>\n      <td>0.361854</td>\n      <td>1.590899</td>\n      <td>5.058090</td>\n      <td>0.957449</td>\n      <td>2.611685</td>\n      <td>746.893258</td>\n      <td>0.938202</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.811827</td>\n      <td>1.117146</td>\n      <td>0.274344</td>\n      <td>3.339564</td>\n      <td>14.282484</td>\n      <td>0.625851</td>\n      <td>0.998859</td>\n      <td>0.124453</td>\n      <td>0.572359</td>\n      <td>2.318286</td>\n      <td>0.228572</td>\n      <td>0.709990</td>\n      <td>314.907474</td>\n      <td>0.775035</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>11.030000</td>\n      <td>0.740000</td>\n      <td>1.360000</td>\n      <td>10.600000</td>\n      <td>70.000000</td>\n      <td>0.980000</td>\n      <td>0.340000</td>\n      <td>0.130000</td>\n      <td>0.410000</td>\n      <td>1.280000</td>\n      <td>0.480000</td>\n      <td>1.270000</td>\n      <td>278.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>12.362500</td>\n      <td>1.602500</td>\n      <td>2.210000</td>\n      <td>17.200000</td>\n      <td>88.000000</td>\n      <td>1.742500</td>\n      <td>1.205000</td>\n      <td>0.270000</td>\n      <td>1.250000</td>\n      <td>3.220000</td>\n      <td>0.782500</td>\n      <td>1.937500</td>\n      <td>500.500000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>13.050000</td>\n      <td>1.865000</td>\n      <td>2.360000</td>\n      <td>19.500000</td>\n      <td>98.000000</td>\n      <td>2.355000</td>\n      <td>2.135000</td>\n      <td>0.340000</td>\n      <td>1.555000</td>\n      <td>4.690000</td>\n      <td>0.965000</td>\n      <td>2.780000</td>\n      <td>673.500000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>13.677500</td>\n      <td>3.082500</td>\n      <td>2.557500</td>\n      <td>21.500000</td>\n      <td>107.000000</td>\n      <td>2.800000</td>\n      <td>2.875000</td>\n      <td>0.437500</td>\n      <td>1.950000</td>\n      <td>6.200000</td>\n      <td>1.120000</td>\n      <td>3.170000</td>\n      <td>985.000000</td>\n      <td>2.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>14.830000</td>\n      <td>5.800000</td>\n      <td>3.230000</td>\n      <td>30.000000</td>\n      <td>162.000000</td>\n      <td>3.880000</td>\n      <td>5.080000</td>\n      <td>0.660000</td>\n      <td>3.580000</td>\n      <td>13.000000</td>\n      <td>1.710000</td>\n      <td>4.000000</td>\n      <td>1680.000000</td>\n      <td>2.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "features = pd.DataFrame(data=raw_data['data'],columns=raw_data['feature_names'])\n",
    "data = features\n",
    "data['target']=raw_data['target']\n",
    "\n",
    "data['class']=data['target'].map(lambda ind: raw_data['target_names'][ind])\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "124"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "data_train, data_test, label_train, label_test = train_test_split(raw_data['data'],raw_data['target'],\n",
    "test_size=0.3)\n",
    "\n",
    "len(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_classifiers = {\n",
    "    \"Logistic Regression\": \n",
    "            {'classifier': LogisticRegression(),\n",
    "                'params' : [\n",
    "                            {\n",
    "                             'penalty': ['l1','l2'],\n",
    "                             'C': [0.001,0.01,0.1,1,10,100,1000]\n",
    "                            }\n",
    "                           ]\n",
    "            },\n",
    "    \"Nearest Neighbors\": \n",
    "            {'classifier': KNeighborsClassifier(),\n",
    "                 'params': [\n",
    "                            {\n",
    "                            'n_neighbors': [1, 3, 5, 10],\n",
    "                            'leaf_size': [3, 30]\n",
    "                            }\n",
    "                           ]\n",
    "            },\n",
    "             \n",
    "    \"Linear SVM\": \n",
    "            {'classifier': SVC(),\n",
    "                 'params': [\n",
    "                            {\n",
    "                             'C': [1, 10, 100, 1000],\n",
    "                             'gamma': [0.001, 0.0001],\n",
    "                             'kernel': ['linear']\n",
    "                            }\n",
    "                           ]\n",
    "            },\n",
    "    \"Gradient Boosting Classifier\": \n",
    "            {'classifier': GradientBoostingClassifier(),\n",
    "                 'params': [\n",
    "                            {\n",
    "                             'learning_rate': [0.05, 0.1],\n",
    "                             'n_estimators' :[50, 100, 200],\n",
    "                             'max_depth':[3,None]\n",
    "                            }\n",
    "                           ]\n",
    "            },\n",
    "    \"Decision Tree\":\n",
    "            {'classifier': tree.DecisionTreeClassifier(),\n",
    "                 'params': [\n",
    "                            {\n",
    "                             'max_depth':[3,None]\n",
    "                            }\n",
    "                             ]\n",
    "            },\n",
    "    \"Random Forest\": \n",
    "            {'classifier': RandomForestClassifier(),\n",
    "                 'params': {}\n",
    "            },\n",
    "    \"Naive Bayes\": \n",
    "            {'classifier': GaussianNB(),\n",
    "                 'params': {}\n",
    "            }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=1, train_sizes=np.linspace(.6, 1.0, 5)):\n",
    "    \"\"\"\n",
    "    Generate a simple plot of the test and traning learning curve.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
    "        An object of that type which is cloned for each validation.\n",
    "\n",
    "    title : string\n",
    "        Title for the chart.\n",
    "\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Training vector, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "\n",
    "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
    "        Target relative to X for classification or regression;\n",
    "        None for unsupervised learning.\n",
    "\n",
    "    ylim : tuple, shape (ymin, ymax), optional\n",
    "        Defines minimum and maximum yvalues plotted.\n",
    "\n",
    "    cv : integer, cross-validation generator, optional\n",
    "        If an integer is passed, it is the number of folds (defaults to 3).\n",
    "        Specific cross-validation objects can be passed, see\n",
    "        sklearn.cross_validation module for the list of possible objects\n",
    "\n",
    "    n_jobs : integer, optional\n",
    "        Number of jobs to run in parallel (default 1).\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classifiers = len(dict_classifiers.keys())\n",
    "\n",
    "def batch_classify(X_train, Y_train, X_test, Y_test, verbose = True):\n",
    "    df_results = pd.DataFrame(\n",
    "        data=np.zeros(shape=(num_classifiers,4)),\n",
    "        columns = ['classifier',\n",
    "                   'train_score', \n",
    "                   'test_score',\n",
    "                   'training_time'])\n",
    "    count = 0\n",
    "    for key, classifier in dict_classifiers.items():\n",
    "        t_start = time.clock()\n",
    "        grid = GridSearchCV(classifier['classifier'], \n",
    "                      classifier['params'],\n",
    "                      refit=True,\n",
    "                        cv = 10, # 9+1\n",
    "                        scoring = 'accuracy', # scoring metric\n",
    "                        n_jobs = -1\n",
    "                        )\n",
    "        estimator = grid.fit(X_train,\n",
    "                             Y_train)\n",
    "        t_end = time.clock()\n",
    "        t_diff = t_end - t_start\n",
    "        train_score = estimator.score(X_train,\n",
    "                                      Y_train)\n",
    "        test_score = estimator.score(X_test,\n",
    "                                     Y_test)\n",
    "        df_results.loc[count,'classifier'] = key\n",
    "        df_results.loc[count,'train_score'] = train_score\n",
    "        df_results.loc[count,'test_score'] = test_score\n",
    "        df_results.loc[count,'training_time'] = t_diff\n",
    "        if verbose:\n",
    "            print(\"trained {c} in {f:.2f} s\".format(c=key,\n",
    "                                                    f=t_diff))\n",
    "        count+=1\n",
    "        plot_learning_curve(estimator, \n",
    "                              \"{}\".format(key),\n",
    "                              X_train,\n",
    "                              Y_train,\n",
    "                              ylim=(0.75,1.0),\n",
    "                              cv=10)\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'batch_classify' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-f91424ecbaaf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_classify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test_score'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'batch_classify' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "df_results = batch_classify(data_train, label_train, data_test, label_test)\n",
    "display(df_results.sort_values(by='test_score', ascending=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}